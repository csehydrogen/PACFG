\section{Introduction}

Deep neural networks (DNNs) have been very successful on various machine learning tasks, such as visual recognition\cite{krizhevsky2012imagenet,vgg,RCNN}, speech recognition\cite{speech}, and machine translation\cite{machinetranslation}.
Among others, the convolutional neural network (CNN) proposed by LeCun \textit{et al.}\cite{726791}, is one of the earliest successful DNN models that were used to classify images.
CNN models equipped with deep learning techniques (\textit{e.g.}, ReLU activation, dropout layers, data augumentation, etc.) outperform previous machine learining techniques in various visual recognition challenges, such as ILSVRC\cite{DBLP:journals/corr/RussakovskyDSKSMHKKBBF14} and PASCAL\cite{pascal}.
The classification accuracy of CNN models have been improving over time.
In addition, recent state-of-the-art techniques for visual recognition use an ensemble of different CNN models\cite{ILSVRC15}.
CNN models are also being applied to other machine learning tasks than image recognition.
They can also be applied to action recognition\cite{actionrecognition}, speech recognition\cite{speech}, natural language processing\cite{DBLP:journals/corr/KalchbrennerGB14}, and playing Go\cite{alphago}.

The biggest advantage of using DNN is its scalability.
A larger and deeper DNN with more parameters usually results in a better accuracy.
However, a larger DNN require more processing power, and training it using a typical computer is impractical.
Fortunately, computations in a neural network can easily be represented as tensor or matrix operations that can be efficiently parallelized.
Thus, GPUs' massively parallel processing power make DNNs to be trained efficiently even in a single desktop computer.
Since deep learning frameworks have been developed for the efficient and easy implementation of DNN models, most of popular deep learning frameworks support GPU acceleration by default\cite{DBLP:journals/corr/Al-RfouAAa16,jia2014caffe,tensorflow2015-whitepaper,torch,cntk}.
As a result, companies and researchers have been trying to implement efficient GPU kernels to improve the performance of deep learning frameworks.

The most popular deep learning library for such frameworks is cuDNN\cite{cudnn}.
It has been developed by NVIDIA, and most of the popular deep learning frameworks use it as the backend for GPUs.
For example, computing convolution with cuDNN results in up to 4X performance improvement compared with the default GPU kernels of the frameworks\cite{convnet-benchmarks}.
Another approach to speed up CNNs is reducing the time complexity of convolution algorithms.
Fast Fourier Transform (FFT) algorithms\cite{fftconv, fbfft} and Winograd's minimal filtering algorithm\cite{winograd} successfully reduce the time complexity of the convolution computation.
However, while the efficiency of deep learning on a single GPU has been improved a lot, deep learning on multiple GPUs still shows poor scalability\cite{DBLP:journals/corr/YadanATR13}.

In this paper, we analyze the performance characteristics of CNNs on different deep learning frameworks and libraries.
For clarity, a \textit{framework} refers to a full collection of libraries to build DNN models and a \textit{library} refers to a GPU kernel library such as cuDNN used for the framework.
We choose five most popular deep learning frameworks, Caffe\cite{jia2014caffe}, CNTK\cite{cntk}, TensorFlow\cite{tensorflow2015-whitepaper}, Theano\cite{DBLP:journals/corr/Al-RfouAAa16}, and Torch\cite{torch}.
The criterion for popularity is the number of github\cite{github} stars.
We also choose a CNN model, AlexNet\cite{krizhevsky2012imagenet}, to obtain the performance characteristics from the frameworks and libraries.
Identically structured AlexNet models are built and trained using these frameworks.
All the five frameworks use cuDNN as the GPU backend.
Cuda-convnet\cite{cuda-convnet} developed by Alex Krizhevsky is another GPU library used in this paper.
In addition, we compare three different convolution algorithms of cuDNN with the direct convolution algorithm of cuda-convnet.

The contributions of this work can be divided into three parts.
First, we analyze differences in the performance characteristics of the frameworks.
We identify performance limiting factors of each framework and describe pros and cons of using each deep learning framework for propspective users.
The performance limiting factors would also be helpful to optimizing the frameworks.
Second, we obtain the performance characteristics of different convolution algorithms.
Based on the result of analyzing the charateristics, we provide possible optimization techniques to implement efficient CNN models using the libraries.
Finally, we obtain the performance characteristics of the frameworks in multi-GPU contexts. We also provide possible techniques to improve the scalability of the frameworks when using multiple GPUs.
