\section{Conclusions and Future work}
In this study, we charaterize deep learning frameworks and libraries to build CNN models using AlexNet trained on a single GPU and multiple GPUs. The performance characteristics indicates that \Comment{==== fill in here ===} By just adjusting options provided by the frameworks, we achieve about 2X speedup based on the observation obtained by the characterization. We also compare different convolution algorithms to build a CNN model using AlexNet. Direct convolution is faster than the convolution computation using GEMM in forward computation, but its backward computation performance is very poor. Using FFT in convolution is usually the fastest. However, it takes up about 20\% more GPU memory space than others. Since the performance of FFT does not depend on the convolution filter size, FFT can speed up convolution layers with a filter size bigger than $5 \times 5$ significantly. However, Winograd convolution is faster than FFT with $3 \times 3$ filters and with small batch size. Our Multi-gpu analysis indicates that the scalability of data paralellism in CNN models is bounded by the communication overhead of network parameters between GPUs. Thus, reducing the amount of parameter transfer would achieve better scalability when expliting data parallelism. Techniques reducing the number of network parameters by pruning the neural network or quantizing the values might improve the scalability of CNN models on multi-GPUs. 
