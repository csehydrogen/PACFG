\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{graphicx}
\usepackage{tabularx, booktabs}
\hyphenation{op-tical net-works semi-conduc-tor}

\begin{document}

\title{Performance Analysis of CNN Frameworks\\for GPUs}

\author{\IEEEauthorblockN{HeeHoon Kim}
\IEEEauthorblockA{Department of Computer Science and Engineering,\\
Seoul National University, Korea\\
Email: hydrogen@snu.ac.kr}
\and
\IEEEauthorblockN{Hyoungwook Nam}
\IEEEauthorblockA{Department of Computer Science and Engineering,\\
Seoul National University, Korea\\
Email: hwnam831@snu.ac.kr}}

\maketitle

\begin{abstract}
Convolutional neural network have been successful in visual recognition tasks.
Machine learning frameworks are using GPU deep learning libraries to efficiently implement CNNs on GPUs.
This study analyzes GPU performance characteristics of Theano, Caffe, Torch and Tensorflow via benchmarking AlexNet model on them.
We also identify differences between 4 convolution algorithm kernels.

\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}

\subsection{Convolutional Neural Network}
Deep neural networks(DNN) have been very successful on various machine learning tasks such as visual recognition, speech recognition and machine translation.
\cite{}%% DNN papers
Convolutional neural network(CNN), proposed by LeCun et. al., is one of the earliest successful neural network model used for image classification tasks.
\cite{}%% lenet
CNN combined with DNN breakthroughs (e.g. ReLU activation, Dropout layer, Data augumentation) outperformed previous machine learining methods on visual recognition challenge.
\cite{}%% alexnet, googlenet, VGG
Classification accuracy of CNN models have been improving over time and recent state-of-the-art programs for visual recognition tasks uses ensembles of CNN models.
\cite{}%% ILSVRC 2016 winner
CNN models are also being applied to other machine learning tasks other than image recognition. CNN can be applied to action recognition, speech recognition, natural language processing and AI playing game of Go.
\cite{}%% one of each

\subsection{Deep learning and gpu}
The biggest advantage of applying DNN is its scalability, means that larger and deeper DNN with more input data usually results in better accuracy.
Bigger DNN requires more processing power, hence training DNN by typical computer is impractical.
Luckily, computations in neural network can easily be represented as tensor or matrix operations, which can be efficiently parallelized.
Therefore parallel processing by GPU made DNN possible to train on single machine.
Deep learning frameworks have been developed for easy and efficient implementation of DNN models, and most popular frameworks support GPU acceleration by default.
\cite{} %% Caffe, Theano, Torch, CNTK, Tensorflow
Companies and researchers have been trying to implement efficient GPU kernels to improve performance.
\cite{} %% fft-conv, fbcunn, Winograd, Cuda-convnet
The most popular one is CuDNN, a GPU deep learning library created by NVIDIA that has been adopted as GPU-backend of popular frameworks.
\cite{} %% CuDNN
Convolution with CuDNN results in up to 4x performance improvement from default GPU kernels of the frameworks.
\cite{} %% convnet-benchmark
Efficiency of deep learning on single GPU has been improved a lot, however deep learning on multiple GPU still shows poor scalability.
\cite{} %% model parallelism, distributed deep learning

\subsection{Motivation}
In this study, we analyze GPU performance characteristics of convolutional neural network on different frameworks or libraries.
We choose 4 most popular deep learning frameworks, Theano, Caffe, Torch and Tensorflow by the number of github stars.
We compare differences in performance characteristics of the frameworks and identify performance limiters of each framework.
The comparison would find out detailed pros and cons of deep learning frameworks and suggest possible improvements.
Furthermore, we take a close look into GPU kernels to analyze performance characteristics of different convolution algorithms.
Based on the analysis, we provide optimization methods to implement efficient CNN model using the GPU deep learning libraries.
Also we analyze performance characteristics of the frameworks on multi-gpu context to suggest efficient implementation method to use multiple gpus.

\section{Related work}

\subsection{Previous work (comparative study, convnet benchmark, cudnn) - no detailed analysis}
It has been only a few years since deep learning frameworks were introduced to public.
Few attempts recently tried to benchmark and compare the performance of the frameworks.
A benchmark of CNN frameworks is publicly available on Github but it has not been updated for more than an year hence it cannot be used to compare current versions of the frameworks \cite{}.
Detailed benchmark of deep learning frameworks was published by Bahrampour et. al. but it only compares frameworks by execution time.
Hence causes for the performance difference were not identified.
Also the benchmark uses older versions of CuDNN which do not support current convolution algorithms such as fft convolution and winograd convolution.
This study, however, compares newest versions of the frameworks and provide detailed analysis down to GPU kernel level.
And also we compare them on multi-gpu context which has not been done in previous studies.


\section{Background (DNN Frameworks)}

\subsection{Machine learning frameworks}

\subsubsection{Theano, Torch, Caffe, Tensorflow comparison}

\subsection{Convolution algorithms}
Several methods are used to efficiently implement convolution on GPU.
Direct convolution is the most straightforward way but needs a lot of specialized kernels to optimize for various input dimensions and corner cases.
Cuda-convnet \cite{} is the efficient direct convolution library written by Alex Krizhevsky, the author of AlexNet paper.
CuDNN however, treats convolution as matrix multiplication \cite{}.
The convolution layer of K kernels with dimension R*R and W*W input with C channels is converted to multiplication of K*CRR filter matrix and CRR*NWW data matrix.
The dimensions of matrices are very big, hence the multiplication can be parallelized using highly efficient BLAS libraries.
Converting convolution to matrix might require significant amount of memory bandwidth.
However, CuDNN computes the multiplication by tiles to hide memory latency while computing \cite{}.
This method scales well on small batch sizes and can be used on all types of convolution layers.
The complexity of both methods are basically the same.

FFT convolution uses fast Fourier transform algorithm to reduce algorithm complexity \cite{}.
FFT significantly reduces the amount of workload but requires much more memory space since filters must be padded to the dimension of inputs.
However, FFT convolution cannot be applied to convolution with stride more than 1.
Winograd convolution algorithm is based on convolution as matrix multiplication but reduces algorithm complexity using Winograd's minimal filtering algorithm.
Winograd’s minimal filtering algorithm follows the idea of Strassen’s algorithm.
For example, minimal Winograd filter of 2x3 tiling reduces number of matrix multiplication from 6 to 4 \cite{}.
However, different sized kernel needs its own minimal filtering algorithm, hence CuDNN 5.0 only supports Winograd convolution for filter size of 3x3 \cite{}.

\subsection{Multi-gpu parallelism}
Multi-gpu implementation of deep neural networks can be implemented by data parallelism or model parallelism \cite{}.
On data parallelism, a batch of inputs is divided and distributed among devices.
After backpropagation, the entire gradients of network parameters must be passed to single device in order to compute stochastic gradient descent.
And then the updated parameters are distributed among devices.
Hence, the communication cost of data parallelism depends on number of parameters in the network.
AlexNet has 65M parameters, thus each iteration needs to transfer approximately 520MB of data per GPU.
On the other hand, model parallelism divides and distributes the network.
Carefully designed model parallelism of convolution layer outperforms the data parallelism \cite{}.
However, multi-gpu support on Caffe is limited to data parallelism, therefore we only compare data parallelism efficiencies of the frameworks.
Tensorflow and Torch supports both data and model parallelism while Theano doesn’t support multi-gpu natively.

\section{Experiment setup}

\subsection{test environment}
We test the frameworks on the CentOS 7.2 server with 4 octa-core Xeon-E5 cpus and 4 GTX TITAN X(GM200) gpus.
We use Cuda 7.5 and CuDNN R5 which is the latest stable release of Cuda and CuDNN.
All deep learning frameworks are updated to latest stable release on June 2016.
The versions of the frameworks fully support CuDNN R5.
Only Torch supports the latest version of Cuda-convnet3.
The detailed system environments are represented on Table \ref{} and \ref{}.

CentOS 7.2.1511 / Linux 3.10.0-327 / Inte Xeon E5-2650@2.0GHz / 128GB DDR3@1600MHz / CuDNN 5005 / Cuda 7.5 / Torch 7 ccn2.torch, cudnn.torch R5 / Theano 0.8.2 / Caffe * / Tensorflow *

\subsection{Network model and input}
AlexNet \cite{} is one of the earliest successful deep neural networks on image recognition task using ImageNet dataset \cite{}.
AlexNet uses 5 convolution layers to extract features and 3 fully connected layers for classification.
Each layer has rectified linear unit(ReLU) layer for nonlinear activation.
AlexNet has been frequently used for benchmarking performance of machine learning libraries, because it utilizes most of the current DNN components such as convolution, max-pooling and dropout \cite{}.
The original AlexNet model includes Local response normalization(LRN) layer, but we exclude it for benchmarking task since LRN is very rarely used in current convolutional neural networks.
The detailed layer structure of AlexNet model on this study is presented in Fig \ref{} and Fig \ref{}.


\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./figures/alexnet_conv.jpg}
  \caption{Convolutional feature extractor part of AlexNet model. C:input channel, K:output channel, R:kernel dimension, S:stride, P:padding}
  \label{fig_alexnet_conv}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./figures/alexnet_fc.jpg}
  \caption{Fully-connected layers of AlexNet model. The input is the output of Convolutional layers}
  \label{fig_alexnet_fc}
\end{figure}

\begin{itemize}
  \item output dimension, number of operation, activation in each layer
  \item dataset : ILSVRC 2012
\end{itemize}

\section{Results/Analysis}

\subsection{Single GPU analysis}

\begin{itemize}
  \item batch : 32, 64, 128
  \item Total running times, Achieved accuracy (default option, optimized option (specify convolution algorithm))
  \item FileIO, CPU, GPU, For/Backward, Layerwise (time / flops / memory)
  \item Framework overhead
\end{itemize}

\subsection{characterization of different convolution algorithms (kernels)}
Since forward and backward propagation of convolution layers takes most of the running time, we run the same model on different convolution kernel to characterize the performance of each convolution algorithm.
Three types of convolutions are computed for each iteration.
forward convolution(FWD) computes the layer output, backward data convolution(BD) computes backward gradient input and backward filter convolution(BW) computes gradients of network parameters.
CuDNN R5 supports matrix multiplication convolution(gemm), FFT convolution, and winograd convolution.
CuDNN has various gemm convolution algorithms and the tested algorithm is implicit gemm precomp algorithm.
Winograd convolution cannot be applied to BW convolution on CuDNN 5.0 hence we use fft convolution instead.(CuDNN 5.1RC supports winograd nonfused option)
Since the first convolution layer has stride of 4, Winograd and FFT convolution cannot be applied.
Direct convolution is tested by Torch binding of Cuda-convnet3.
All comparisons are done on Torch 7 because Torch can specify convolution algorithm on each layer and newest version of Cuda-convnet3 is only supported by Torch7.

Randomly generated batch inputs are used to remove IO latency.
The forward and backward propagation time is measured as average of 100 iterations.
The compute times and statistics of kernels are measured by NVIDIA nvprof profiler.
The theoretical floating point operation counts are calculated as 2 * K*CRR*NWW since each calculation uses 1 addition and 1 multiplication.
We compare them to actual floating-point operation counts of the kernels.
Flops of the kernels are calculated as Flop count/execution time.
FFT convolution consists of 2 FFTs and 1 complex matrix multiplication, thus statistics of those 3 kernels are added together.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{./figures/conv_time}
  \caption{Forward propagation and Backpropagation times of convolution algorithms. The times are measured as average of 100 batch iterations. 
Direct convolution is tested with cuda-convnet3 and other 3 are kernels of CuDNN R5. }
  \label{fig_conv_time}
\end{figure}

FIg \ref{fig_conv_time} shows execution time comparisons between convolution kernels.
Winograd convolutoin and FFT convolution always perform better than direct or gemm convolution.
Winograd performs better than FFT in small batches, while FFT scales better on bigger batch inputs.
Cuda-convnet shows poor performance on backpropagation.
The performance limiter of Cuda-convnet backpropation would be identified on layer-wise analysis.



\begin{itemize}
  \item theoretical flops, real flops
  \item memory footprint, compute utilization (Memory transactions, Control-instructions, IPC, Memory usage)
  \item difference between algorithms
\end{itemize}

\subsection{Multi GPU analysis}

\begin{itemize}
  \item Support
  \item Scalability : proportion of data exchange
  \item Synchronization cost
\end{itemize}

\section{Discussion / Conclusion}

\begin{itemize}
  \item Summary of results
  \item Compare frameworks : Implementation difference
  \item Locate bottleneck / suggest possible optimization
  \item Limitation, future research
\end{itemize}

\section*{Acknowledgment}

The authors would like to thank...

\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}

\end{document}
