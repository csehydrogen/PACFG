\documentclass{abstract_hutech}

\usepackage{cite}
\usepackage{graphicx}
\usepackage{tabularx, booktabs}
\usepackage{subfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\hyphenation{op-tical net-works semi-conduc-tor}

\newcommand{\Comment}[1]{}

\begin{document}
\thispagestyle{firstpage}
\twocolumn[
\begin{@twocolumnfalse}
\vspace*{20pt}
\begin{flushleft}
\fontsize{20}{0}\selectfont{\textbf{Performance Analysis of CNN Frameworks\\
 for GPUs}}
\vspace{32pt}\par
\fontsize{10}{12}\selectfont{\textbf{In this paper, we analyze the GPU performance characteristics of five popular deep learning frameworks: Caffe, CNTK, TensorFlow, Theano, and Torch in the perspective of a representative CNN model, AlexNet. Based on the characteristics obtained, we suggest possible optimization methods to increase the efficiency of CNN models built by the frameworks. We also show the GPU performance characteristics of different convolution algorithms each of which uses one of GEMM, direct convolution, FFT, and the Winograd method. We also suggest criteria to choose convolution algorithms for GPUs and methods to build efficient CNN models on GPUs. We also analyze the scalability of the CNN models built by the deep learning frameworks in the multi-GPU context and their overhead. The result indicates that we can increase the speed of training the AlexNet model up to 2X by just changing options provided by the frameworks. }}
\end{flushleft}
\vspace{20pt}
\end{@twocolumnfalse}
]

\renewcommand{\baselinestretch}{.95} 
\input{introduction_small}
\input{comparison_small}
\input{algorithms_small}
\input{multigpu_small}
\input{conclusion}


%\section*{Acknowledgment}

%The authors would like to thank...

\bibliographystyle{humantech}
\bibliography{ispass17}
%\nocite{*}

\end{document}
