Thanks to modern deep learning frameworks that exploit GPUs, convolutional neural networks (CNNs) have been greatly successful in visual recognition tasks. In this paper, we analyze the GPU performance characteristics of five popular deep learning frameworks: Caffe, CNTK, TensorFlow, Theano, and Torch in the perspective of a representative CNN model, AlexNet. We identify the overhead of each framework and suggest possible optimization methods to increase the efficiency of CNN models built by the framework. We also show the GPU performance characteristics of four convolution algorithms each of which uses one of GEMM, direct convolution, FFT, and Winograd method. Based on the characterization, we suggest criteria to choose convolution kernels for GPUs and methods to build efficient CNN models on GPUs. Since scaling DNNs in a multi-GPU context becomes important, we also analyze the scalability of the deep learning frameworks in the multi-GPU context and their overheads. Based on the multi-GPU characterization, we suggest possible optimization methods for CNNs in the multi-GPU context.