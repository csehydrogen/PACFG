\section{Introduction}
Deep neural networks (DNNs) have been very successful in various machine learning tasks, such as visual recognition\cite{krizhevsky2012imagenet,vgg,RCNN}, speech recognition\cite{speech}, and machine translation\cite{machinetranslation}. Among others, the convolutional neural network (CNN) proposed by LeCun \textit{et al.}\cite{726791} is one of the earliest successful DNN models that were used to classify images. A larger and deeper CNN with more parameters usually results in a better accuracy. Thus, GPUs' massively parallel processing power makes CNNs to be trained efficiently. However, while the efficiency of a CNN on a single GPU has been improved a lot, its efficiency on multiple GPUs still shows poor scalability\cite{DBLP:journals/corr/YadanATR13}. 

In this paper, we analyze the performance characteristics of CNN models built with different deep learning frameworks and libraries. We choose five most popular deep learning frameworks, Caffe\cite{jia2014caffe}, CNTK\cite{cntk}, TensorFlow\cite{tensorflow2015-whitepaper}, Theano\cite{DBLP:journals/corr/Al-RfouAAa16}, and Torch\cite{torch}. We choose a representative CNN model, AlexNet\cite{krizhevsky2012imagenet}, to compare the five frameworks and to obtain performance characteristics.  Our comparative study provides useful insights to both end users and developers of a DNN framework. We identify which part of the GPU implementation of a framework is the performance bottleneck. This provides the developers with optimization possibilities. This will also help end users to choose a proper framework for their CNN models. 



